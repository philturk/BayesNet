---
title: "Model - Power Law"
author: "Gabrielle Lemire"
date: "3/12/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rlist)
library(reshape2)
library(rstanarm)
library(boot)
library(poweRlaw) #compare dists
library(GLDEX) #remove zeros
# load data
# load objects created in "sampling_sim.R"
sim_dist_100 = read.csv("C://Code//msu//gra//BayesNet//Paper8//sim_dists.csv")

# --------------- MELT ALL PROPORTIONS
melt_all_props = function(data = sim_dist_100){
  library(reshape2)
  # create one master df with the following columns:
  # X - the size of the component
  # variable - which sim it came from
  # value - count of components
  
  beg_prop = seq(from = 1, to = 346, by = 23)
  end_prop = seq(from = 23, to = 368, by = 23)
  samp_prop = seq(from = .95, to = 0.2, by = -0.05)
  df = data.frame()
  df_all_sim = data.frame()
  for (i in 1:length(beg_prop)){
    beg = beg_prop[i]
    end = end_prop[i]
    df = as.data.frame(data[beg:end,])
    df$X = c(1:23) #leave as 1 to 23 because all props should have this component count
    df = melt(df, id.vars="X")
    df_all_sim = rbind(df_all_sim, df)
  }
  return(df_all_sim)
}

# melt dataframe with all proportions and simulations into one master df_all_sim
df_all_sim = melt_all_props(data = sim_dist_100)
# define start and end rows in df_all_sim that separate each proportion
# start_row = seq(from = 1, to = 36800, by = 2300)
# end_row = seq(from = 2300, to = 36800, by = 2300)

# data_95 = df_all_sim[start_row[1]:end_row[1],3]
# data_9 = df_all_sim[start_row[2]:end_row[2],3]
# data_85 = df_all_sim[start_row[3]:end_row[3],3]
# data_8 = df_all_sim[start_row[4]:end_row[4],3]
# data_75 = df_all_sim[start_row[5]:end_row[5],3]
# data_7 = df_all_sim[start_row[6]:end_row[6],3]
# data_65 = df_all_sim[start_row[7]:end_row[7],3]
# data_6 = df_all_sim[start_row[8]:end_row[8],3]
# data_55 = df_all_sim[start_row[9]:end_row[9],3]
# data_5 = df_all_sim[start_row[10]:end_row[10],3]
# data_45 = df_all_sim[start_row[11]:end_row[11],3]
# data_4 = df_all_sim[start_row[12]:end_row[12],3]
# data_35 = df_all_sim[start_row[13]:end_row[13],3]
# data_3 = df_all_sim[start_row[14]:end_row[14],3]
# data_25 = df_all_sim[start_row[15]:end_row[15],3]
# data_2 = df_all_sim[start_row[16]:end_row[16],3]

```

# Visuals of Component Distribution for Simulated Genetic Networks
## Overview of work in this File

**Previous version of this file has example with "moby" word frequency dataset found in Gillespie's paper (2015). Since using all the functions in that example we removed from the file (find in previous commit 02fa54f).**

In this file we compare the following discrete distributions' fit to our data (all found within the poweRlaw package).

- $displ$ - power law
- $dislnorm$ - log normal
- $disexp$ - exponential
- $dispois$ - poisson

We have 12 possible comparisons per proportion:
- PL vs LN, PL vs exp, PL vs Pois
- LN vs PL, LN vs exp, LN vs Pois
- exp vs PL, exp vs LN, exp vs Pois
- Pois vs PL, Pois vs LN, Pois vs exp
Note: order matters because we'll set our $x_{min}$ based on which one comes first in this order.

**Note: poweRlaw also exists for Python https://github.com/jeffalstott/powerlaw**

Following the above compared fit for one simulation and one proportion, we want to combine the results of all the simulations for one proportion so that we can compare the estimates between proportions.

## Power Law for on Simulation in our Data

```{r, echo=FALSE}
library(GLDEX)

# # other data format
# df_95 = read.csv("C://Code//msu//gra//BayesNet//Paper8//proportions//prop95.csv")
# test = order(df_95$comp_size)
# -----COMPARE PL TO LN USING PL X_MIN
data_95 = df_all_sim[start_row[1]:end_row[1],3]
data = fun.zero.omit(data_95) #function seems to work
data = data[order(-data)]
# length(data)
# data[1400:1442]

pl_95 =  displ$new(test)
# estimate x_min
est_pl_xmin <- estimate_xmin(pl_95) #x_min is 1, gof is .046
pl_95$setXmin(est_pl_xmin$xmin)
# for a given x_min we can estimate alpha w MLE
est_pl_pars = estimate_pars(pl_95) #alpha = 1.46
pl_95$setPars(est_pl_pars$pars)
pl_95
# plot(pl_95) #creates log log of plot - it looks weird
# lines(pl_95, col = 2) #adds the fitted distribution?? not working
# # can save as data to plot with other R packages
# dd = plot(pl_95)
# head(dd,3)
# create lognormal object
ln_95 <- dislnorm$new(test)
# estimate x_min
ln_95$setXmin(est_pl_xmin$xmin) #notice our x_min object is PL
# estimate alpha based on best x_min
est_ln_pars = estimate_pars(ln_95) #alpha = -3.75, 4 as params
ln_95$setPars(est_ln_pars$pars)
ln_95
# compare the two objects we just created
compare_pl_ln = compare_distributions(pl_95, ln_95) 
plot(compare_pl_ln) #no idea what i'm looking at
# interpret output - p5 of cran pdf

# test_statistic The test statistic.

# p_one_sided - A one-sided p-value, which is an upper limit on getting that small a log-likelihood ratio if the first distribution, d1, is actually true.... i.e. if it's actually power law will have a low/high p-value here . p-value = .99??

# p_two_sided - A two-sided p-value, which is the probability of getting a log-likelihood ratio which deviates that much from zero in either direction, if the two distributions are actually equally good.... so low number means they're not equally good. yes.

# ratio - A data frame with two columns. The first column is the x value and second column is the difference in log-likelihoods.
length(compare_pl_ln$ratio) #2? lol


# -----COMPARE PL TO LN USING PL X_MIN
# choosing lognormal to compare fit to
# create lognormal object
ln_95 <- dislnorm$new(test)
# estimate x_min
est_ln_xmin <- estimate_xmin(ln_95) #x_min is 624, gof is .033
ln_95$setXmin(est_ln_xmin$xmin)
# estimate alpha based on best x_min
est_ln_pars = estimate_pars(ln_95) #alpha = 6.46, .011 as params
ln_95$setPars(est_ln_pars$pars)
ln_95
# create pl object with x_min value determined by ln
pl_95 =  displ$new(test)
# estimate x_min
pl_95$setXmin(est_ln_xmin$xmin) #notice our x_min object is LN
# for a given x_min we can estimate alpha w MLE
est_pl_pars = estimate_pars(pl_95) #alpha = 1.46
pl_95$setPars(est_pl_pars$pars)
pl_95

# compare the two objects we just created
compare_ln_pl = compare_distributions(ln_95, pl_95)
# getting this error (not sure why):
# Error in if (p1 < 0.5) { : missing value where TRUE/FALSE needed
# something about the results specific to this
# retry after i change datatype going in


```

Some questions I have: does our x_min or alpha change for diff sampling rates? (I doubt x_min would change, but would be curious)... also if x_min is 1 then are we only looking at the tail where x=1 (because we don't include 0's)?

### Comparing Dists for one Simulation of 95% Sampling Rate

**Notes from group meeting on May 21, 2021**: 

tfidf - zipfl law freq vs rank... common for text analytics
-add 1 to everything... he has seen it done for tfidf
-there are variations on that
-just add 1
-expecting counts
-might extend tail even longer if we multiply

-share with phil and nicole

-like an indicator __species
-idf is inverse document relative freqncies (idf) -- usually cast as natual log of the ratio of # of docs in corpus / num of docs that contain the term... but if nothing then it contains a 0 ... so ppl will replace with a 1... but th way he teaches it ... polyamorous - a very common term that appears many times in many docs, and the upper extreme is going to be bounded by the ln(idf), and that term would be an indicator term or monogamous (very special and a characteristic signature for that corpus)...


- does it maintain dist, do params change, etc as sample rate changes
- in survival analysis - exp, weibull ,ln, etc... typical sop is likelihood ratio to compare pairs & proc life reg for gof of each model
prob/proc __ metric -- to see what fit is like... generates a prob-prob or quant-quant plot, uses a mod of Kaplan-Meyer to compare points to line...
- Nicole - sow e'd be thinking of cluster size as survival time... 

- do we care about drop off of 1's
- diff sim's

- one or two page exec summary - Phil is interested
- include that as a question -- send paper about missing data on linkage rates from Nicole
- make plan to meet Phil, Nicole & I

```{r}
data_1 = df_all_sim[1:23,3]
data_1 = fun.zero.omit(data_1) #function seems to work
data_1 = data_1[order(-data_1)]

# use to plug in
model_pl = displ$new(data_1)
model_ln = dislnorm$new(data_1)
model_exp = disexp$new(data_1)
model_pois = dispois$new(data_1)

# double check that both give 22 as x_min, yes they do
# testexp = disexp$new(data_1)
# estimate_xmin(testexp) #gof is .38
# testpois = dispois$new(data_1)
# estimate_xmin(testpois) #gof is .67


# compare others to PL
compare_models = function(model_1, model_2, model_1_name = "model_1_name", model_2_name = "model_2_name", prop = "00%"){
  library(poweRlaw)
  # data - is a dataframe with frequencies of 
  # component sizes or degrees (no zeros)
  # model_object - as initiated by displ, dislnorm, disexp, or dispois
  # model_name - is a string to ease interpretation of print outs
  # x_min is determined by the pl object 
  # since x_min for both models must be the same
  print(paste("------------------", model_1_name, "versus", model_2_name, "for data prop", prop))
  print(paste("default x_min of model_1", model_1_name,
              as.character(model_1$xmin)))
  print(paste("default x_min of model_2", model_2_name,
              as.character(model_2$xmin)))
  
  # estimate and set x_min based on PL fit of data
  est_xmin = estimate_xmin(model_1)
  model_1$setXmin(est_xmin$xmin)
  # estimate and set alpha based on x_min
  est_pars_1 = estimate_pars(model_1)
  model_1$setPars(est_pars_1$pars)
  
  print(paste("estimated x_min based on fitting data to model_1",
              model_1_name, as.character(model_1$xmin), 
              "estimated alpha is", as.character(model_1$pars)))
  # set x_min based on PL fit of data
  model_2$setXmin(est_xmin$xmin)
  # estimate and set alpha based on x_min
  est_pars_2 = estimate_pars(model_2)
  model_2$setPars(est_pars_2$pars)
  print(paste("estimated x_min ",
              as.character(model_1$xmin), 
              "is assigned to model_2", model_2_name,
              "estimated alpha is", as.character(model_2$pars)))
  print("with the same x_min we can compare models")
  compare_models = compare_distributions(model_1, model_2)
  print(paste("p value one sided is",
               as.character(compare_models$p_one_sided)))
  print(paste("p value two sided is",
               as.character(compare_models$p_two_sided)))

  return(compare_models)
}

```
```{r}
# POWER LAW compared to x
compare_pl_ln = compare_models(model_1 = model_pl, 
                                model_2 = model_ln, 
                                model_1_name = "powerlaw", 
                                model_2_name = "lognormal", 
                                prop = "95%")

compare_pl_exp = compare_models(model_1 = model_pl, 
                                model_2 = model_exp, 
                                model_1_name = "powerlaw", 
                                model_2_name = "exponential", 
                                prop = "95%")

compare_pl_pois = compare_models(model_1 = model_pl, 
                                model_2 = model_pois, 
                                model_1_name = "powerlaw", 
                                model_2_name = "poisson", 
                                prop = "95%")

# LOG NORMAL compared to x
compare_ln_pl = compare_models(model_1 = model_ln, 
                                model_2 = model_pl, 
                                model_1_name = "lognormal", 
                                model_2_name = "powerlaw", 
                                prop = "95%")

compare_ln_exp = compare_models(model_1 = model_ln, 
                                model_2 = model_exp, 
                                model_1_name = "lognormal", 
                                model_2_name = "exponential", 
                                prop = "95%")

compare_ln_pois = compare_models(model_1 = model_ln, 
                                model_2 = model_pois, 
                                model_1_name = "lognormal", 
                                model_2_name = "poisson", 
                                prop = "95%")

# EXPONENTIAL compared to x
compare_exp_pl = compare_models(model_1 = model_exp, 
                                model_2 = model_pl, 
                                model_1_name = "exponential", 
                                model_2_name = "powerlaw", 
                                prop = "95%")

# disturbing! why is function returning modified model object (default x_min is 22 now?! should be contained in the function/module environment!)
compare_exp_ln = compare_models(model_1 = model_exp, 
                                model_2 = model_ln, 
                                model_1_name = "exponential", 
                                model_2_name = "lognormal", 
                                prop = "95%")

compare_exp_pois = compare_models(model_1 = model_exp, 
                                model_2 = model_pois, 
                                model_1_name = "exponential", 
                                model_2_name = "poisson", 
                                prop = "95%")

# POISSON compared to x
compare_pois_pl = compare_models(model_1 = model_pois, 
                                model_2 = model_pl, 
                                model_1_name = "poisson", 
                                model_2_name = "powerlaw", 
                                prop = "95%")

compare_pois_ln = compare_models(model_1 = model_pois, 
                                model_2 = model_ln, 
                                model_1_name = "poisson", 
                                model_2_name = "lognormal", 
                                prop = "95%")

compare_pois_exp = compare_models(model_1 = model_pois, 
                                model_2 = model_exp, 
                                model_1_name = "poisson", 
                                model_2_name = "exponential", 
                                prop = "95%")
```


```{r}
# rerun previous chunk with modified input by adding 1's
data_2 = df_all_sim[1:23,3] + 1
data_2 = data_2[order(-data_2)]

# use to plug in
model_pl = displ$new(data_2)
model_ln = dislnorm$new(data_2)
model_exp = disexp$new(data_2)
model_pois = dispois$new(data_2)

```

```{r}
library(poweRlaw)
# note to self - if i make into a loop add a vertical line where x_min is

# let's visually compare exp vs PL
# use data_2 i.e. with adding 1

model_exp$setXmin(1)
model_exp$setPars(estimate_pars(model_exp))

model_pl$setXmin(1)
model_pl$setPars(estimate_pars(model_pl))

compare_distributions(model_exp, model_pl)

plot(model_pl, ylab = "CDF", main="Comparing Power Law (red) and Exponential (black), x_min = 1")
lines(model_exp) #fit looks terrible lol
lines(model_pl, col = 2, lty = 2) #fit looks much better

# let's visually compare exp vs LN
# use data_2 i.e. with adding 1

model_exp$setXmin(1)
model_exp$setPars(estimate_pars(model_exp))

model_ln$setXmin(1)
model_ln$setPars(estimate_pars(model_ln))

compare_distributions(model_exp, model_ln)

plot(model_ln, ylab = "CDF", main="Comparing Lognormal (red) and Exponential (black), x_min = 1")
lines(model_exp) #fit looks terrible lol
lines(model_ln, col = 2, lty = 2) #fit looks much better - not as good as PL but why is it hitting the y-axis, x=0?

# let's visually compare PL vs LN for x_min=1
# use data_2 i.e. with adding 1

model_pl$setXmin(1)
model_pl$setPars(estimate_pars(model_pl))

model_ln$setXmin(1)
model_ln$setPars(estimate_pars(model_ln))

compare_distributions(model_pl, model_ln)$p_one_sided
compare_distributions(model_pl, model_ln)$p_two_sided

plot(model_ln, ylab = "CDF", main="Comparing Lognormal (red) and Power Law (black), x_min = 1") #note it's just taking the data so doesn't matter if we plot model_ln or model_pl (based on trying both and it looking the same)
lines(model_pl) #fit looks terrible lol
lines(model_ln, col = 2, lty = 2) #fit looks much better - not as good as PL but why is it hitting the y-axis, x=0?

# two params - combine models?
# ex Yule Simon (extra degree of freedom might be enough to capture)
# can we add the extra data... add simulations
# how to add simulations? ... try to do it just add them as extra points
```

### Interpreting one sided p values

Assuming two sided p value is small enough, low one sided p value means the first model is a better fit.

```{r}
# redo example to confirm underdstanding of one sided p-value
set.seed(1)
x = rpldis(1000, xmin = 2, alpha = 3)
m1 = displ$new(x)
m1$setPars(estimate_pars(m1))
m2 = dispois$new(x)
m2$setPars(estimate_pars(m2))
plot(m2, ylab = "CDF")
lines(m1)
lines(m2, col = 2, lty = 2)
compare_distributions(m1, m2)$p_one_sided #.026 "suggests power-law models gives a better fit" which is m1... so low number means first one is better fit

compare_distributions(m1, m2)$p_two_sided #.05
# means  we  can  rejectH0sincep=  0.05142  and  conclude  that  one  model  is  closer  to  the  truedistribution.
```
## Try adding more simulations

Try summing them. Multiply average by 100.

Some idea of variability if you compare individual sims to the sum of all 100.

Reuben's rules for combining results from multiple estimated parameters. beta hat 1, beta hat 2.... beta hat 100.... we get beta hat = average of beta hats.

variance of all is average of variances + (n+1) var of (beta hats)

create our own plot which plots all of them with combined and some bounds and see what it looks like

```{r}
# rerun previous chunk with modified input by adding 1's
data_3 = df_all_sim[1:69,3] + 1 #try with first three sims... doesn't work
data_3 = data_3[order(-data_3)]

# use to plug in
model_pl = displ$new(data_2)
model_ln = dislnorm$new(data_2)
model_exp = disexp$new(data_2)
model_pois = dispois$new(data_2)

```


**old notes can probably delete but I'm a hoarder**:

-give understanding of yule-simon family next meeting (what is a vglm, etc) & how it relates to glm/what i've seen already

- big question after will be how to move parameters in this situation into bigger epidemic model